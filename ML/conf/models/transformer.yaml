# --- Experiment configurations --------------------------------------------------------------------

# experiment name, used as folder name
experiment_name: transformer

# fixed seed, leave empty to use a random seed
seed: 42

# --- Model configuration --------------------------------------------------------------------------

# base model type [lstm, ealstm, cudalstm, embcudalstm, shortcutlstm, dropoutlstm, cudalstminitialh]
# (has to match the if statement in modelzoo/__init__.py)
model: transformer

statics_embedding:
  type: fc
  hiddens: 
    - 32
  activation: tanh
  dropout: 0.0


dynamics_embedding:
  type: fc
  # define number of neurons per layer in the FC network used as embedding network
  hiddens:
    - 16
  # activation function of embedding network
  activation: tanh
  # dropout applied to embedding network
  dropout: 0.0


output_activation: linear

head: "regression"

transformer_positional_encoding_type: sum
transformer_positional_dropout: 0
transformer_nheads: 4
transformer_dim_feedforward: 32
transformer_dropout: 0
transformer_nlayers: 4

hidden_size: 64

output_dropout: 0.3

# --- Training configuration -----------------------------------------------------------------------

# specify optimizer [Adam, AdamW, Adadelta]
optimizer: Adam

# specify loss [MSE, NSE, RMSE, UMALLoss, MDNLoss]
loss: MSE

# specify learning rates to use starting at specific epochs (0 is the initial learning rate)
learning_rate:
  0: 1e-3
  1: 1e-3
  5: 1e-4
  10: 1e-5
  20: 1e-5

# Mini-batch size
batch_size: 2048

# Number of training epochs
epochs: 20

# Length of the input sequence
seq_length: 365


# --- Data configurations --------------------------------------------------------------------------

# specify dataset version [beta, sigma]
dataset: sigma


# --- Side configurations --------------------------------------------------------------------------

wandb: True

discharge: True

gpu: 0


# --- Hydra configurations -------------------------------------------------------------------------

defaults:
  - _self_
  # - override hydra/hydra_logging: disabled  
  - override hydra/job_logging: disabled 

hydra:
  verbose: True
  sweeper:
    params:
      hidden_size: 64,128,256
      transformer_nheads: 4,8
      transformer_dim_feedforward: 16,32,64
      transformer_dropout: 0,0.3
      transformer_nlayers: 4,8
      seq_length: 180,270,365
      optimizer: Adam,AdamW
      output_dropout: 0.3,0.4

  output_subdir: null  
  run:  
    dir: .