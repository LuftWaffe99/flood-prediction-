{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Jupyter Notebook is to derive the following static catchment attributes (climatographic):\n",
    "\n",
    "-> p_mean \\\n",
    "-> p_seasonality \\\n",
    "-> snow_frac_daily \\\n",
    "-> high_prec_freq \\\n",
    "-> high_prec_dur\\\n",
    "-> low_prec_freq\\\n",
    "-> low_prec_dur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_p_mean(df: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculates mean precipitation for the whole period\"\"\"\n",
    "    # calculate mean precipitations for each year\n",
    "    df = df.groupby(df.date.dt.year)[\"prcp\"].mean()\n",
    "    # return the average\n",
    "    return df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_p_seasonality(df: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculates precipitation seasonality\"\"\"\n",
    "\n",
    "    df[\"day_of_year\"] = df[\"date\"].dt.strftime(\"%m-%d\")\n",
    "\n",
    "    average_prcp = df.groupby(\"day_of_year\")[\"prcp\"].mean().reset_index()\n",
    "    average_prcp.columns = [\"day_of_year\", \"avg_prcp\"]\n",
    "    sorted_days = (\n",
    "        pd.date_range(start=\"2019-05-01\", end=\"2020-04-30\").strftime(\"%m-%d\").tolist()\n",
    "    )\n",
    "    average_prcp[\"day_of_year\"] = pd.Categorical(\n",
    "        average_prcp[\"day_of_year\"], categories=sorted_days, ordered=True\n",
    "    )\n",
    "    average_prcp = average_prcp.sort_values(\"day_of_year\").reset_index(drop=True)\n",
    "\n",
    "    average_t_mean = df.groupby(\"day_of_year\")[\"t_mean\"].mean().reset_index()\n",
    "    average_t_mean.columns = [\"day_of_year\", \"avg_t_mean\"]\n",
    "    average_t_mean = average_t_mean.sort_values(by=\"day_of_year\").reset_index(drop=True)\n",
    "    average_t_mean[\"day_of_year\"] = pd.Categorical(\n",
    "        average_t_mean[\"day_of_year\"], categories=sorted_days, ordered=True\n",
    "    )\n",
    "    average_t_mean = average_t_mean.sort_values(\"day_of_year\").reset_index(drop=True)\n",
    "\n",
    "    extended_prcp = pd.concat([average_prcp] * 3, ignore_index=True)\n",
    "    extended_prcp[\"29_day_avg_prcp\"] = (\n",
    "        extended_prcp[\"avg_prcp\"].rolling(window=29, center=True).mean()\n",
    "    )\n",
    "    centered_29_day_avg = extended_prcp.iloc[366:732].reset_index(drop=True)\n",
    "\n",
    "    extended_t_mean = pd.concat([average_t_mean] * 3, ignore_index=True)\n",
    "    extended_t_mean[\"29_day_avg_prcp\"] = (\n",
    "        extended_t_mean[\"avg_t_mean\"].rolling(window=29, center=True).mean()\n",
    "    )\n",
    "    centered_29_day_avg_t_mean = extended_t_mean.iloc[366:732].reset_index(drop=True)\n",
    "\n",
    "    average_prcp[\"29_day_avg_prcp\"] = centered_29_day_avg[\"29_day_avg_prcp\"]\n",
    "    average_t_mean[\"29_day_avg_t_mean\"] = centered_29_day_avg_t_mean[\"29_day_avg_prcp\"]\n",
    "\n",
    "    df = pd.merge(average_prcp, average_t_mean, on=\"day_of_year\")\n",
    "\n",
    "    t_data = df[\"29_day_avg_t_mean\"].values\n",
    "    p_data = df[\"29_day_avg_prcp\"].values\n",
    "\n",
    "    N = len(t_data)\n",
    "    t = np.linspace(0, 2 * np.pi, N)\n",
    "\n",
    "    t_guess_mean = np.mean(t_data)\n",
    "    t_guess_amp = (np.max(t_data) - np.min(t_data)) / 2\n",
    "    p_guess_mean = np.mean(p_data)\n",
    "    p_guess_amp = (np.max(p_data) - np.min(p_data)) / 2\n",
    "    guess_phase = 0\n",
    "\n",
    "    t_optimize_func = lambda x: x[0] * np.sin(t + x[1]) + x[2] - t_data\n",
    "    p_optimize_func = lambda x: x[0] * np.sin(t + x[1]) + x[2] - p_data\n",
    "\n",
    "    t_est_amp, t_est_phase, t_est_mean = scipy.optimize.leastsq(\n",
    "        t_optimize_func, [t_guess_amp, guess_phase, t_guess_mean]\n",
    "    )[0]\n",
    "    p_est_amp, p_est_phase, p_est_mean = scipy.optimize.leastsq(\n",
    "        p_optimize_func, [p_guess_amp, guess_phase, p_guess_mean]\n",
    "    )[0]\n",
    "\n",
    "    p_est_amp = p_est_amp / p_est_mean\n",
    "\n",
    "    p_seasonality = (\n",
    "        p_est_amp * t_est_amp / abs(t_est_amp) * np.cos(p_est_phase - t_est_phase)\n",
    "    )\n",
    "\n",
    "    return p_seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_snow_frac_daily(df: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculates the fraction of snow precipitation to the total precipitations for the whole period\"\"\"\n",
    "    # take days where max temperature were less than 0\n",
    "    df_snow_frac_daily = df[df[\"t_mean\"] < 0]\n",
    "    # take rows where precipitation is not NaN and more than 0\n",
    "    df_snow_frac_daily = df_snow_frac_daily[\n",
    "        (df_snow_frac_daily[\"prcp\"].notna()) & (df_snow_frac_daily[\"prcp\"] > 0)\n",
    "    ]\n",
    "    # compute the fraction of snow to the total precipitations amount\n",
    "    df_snow_frac_daily = df_snow_frac_daily[\"prcp\"].sum() / df[\"prcp\"].sum()\n",
    "    return df_snow_frac_daily\n",
    "    # print(df.head())\n",
    "    # mean_monthly_temp = df[\"t_mean\"].groupby(df.Mnth).mean()\n",
    "    # mean_monthly_precip = df[\"prcp\"].groupby(df.Mnth).mean()\n",
    "    # return mean_monthly_precip.loc[mean_monthly_temp < 0].sum() / mean_monthly_precip.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aridity(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Calculates aridity as ratio of `pet_mean` to `p_mean`\"\"\"\n",
    "    return df[\"pet_mean\"] / df[\"p_mean\"]\n",
    "\n",
    "\n",
    "def calc_high_prec_freq(df: pd.DataFrame, p_mean: float) -> float:\n",
    "    \"\"\"Calculates the number of high precipitation days and averages across years\"\"\"\n",
    "    # take the days where precipitations were 5 times the precipitation daily mean\n",
    "    high_prec_freq_df = df[df[\"prcp\"] >= 5 * p_mean]\n",
    "    # obtain the number of such days for each year\n",
    "    high_prec_freq = high_prec_freq_df.groupby(high_prec_freq_df.date.dt.year).size()\n",
    "    # return the average across years\n",
    "    return high_prec_freq.mean()\n",
    "\n",
    "\n",
    "def calc_high_prec_dur(df: pd.DataFrame, p_mean: float) -> float:\n",
    "    \"\"\"Calculates the average duration of high precipitation days across all years\"\"\"\n",
    "    # filter rows by the amount of precipitations\n",
    "    df = df[df[\"prcp\"] >= 5 * p_mean]\n",
    "    s = df.groupby(\"year\").date.diff().dt.days.ne(1).cumsum()\n",
    "    df = df.groupby([\"year\", s]).size().reset_index(level=1, drop=True)\n",
    "    # find average consecutive days for each year\n",
    "    df_high_prec_dur_years = df.groupby(\"year\").mean()\n",
    "    # return average consecutive days across all years\n",
    "    return df_high_prec_dur_years.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_low_prec_freq(df: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculates the number of dry days and averages across years\"\"\"\n",
    "    # take the days where precipitations were less than 1 mm/day\n",
    "    low_prec_freq_df = df[df[\"prcp\"] < 1]\n",
    "    # obtain the number of such days for each year\n",
    "    low_prec_freq = low_prec_freq_df.groupby(low_prec_freq_df.date.dt.year).size()\n",
    "    # return the average across years\n",
    "    return low_prec_freq.mean()\n",
    "\n",
    "\n",
    "def calc_low_prec_dur(df: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculates the average duration of high precipitation days across all years\"\"\"\n",
    "    # filter rows by the amount of precipitations\n",
    "    df = df[df[\"prcp\"] < 1]\n",
    "    s = df.groupby(\"year\").date.diff().dt.days.ne(1).cumsum()\n",
    "    df = df.groupby([\"year\", s]).size().reset_index(level=1, drop=True)\n",
    "    # find average consecutive days for each year\n",
    "    df_low_prec_dur_years = df.groupby(\"year\").mean()\n",
    "    # return average consecutive days across all years\n",
    "    return df_low_prec_dur_years.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_basin(df: pd.DataFrame, basin_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Calculates static attributes for a given basin\"\"\"\n",
    "\n",
    "    # add new column `year`\n",
    "    df_year = df.copy()\n",
    "    df_year[\"year\"] = df[\"date\"].dt.year\n",
    "\n",
    "    df = df_year\n",
    "\n",
    "    # start calculations\n",
    "    p_mean = calc_p_mean(df)\n",
    "    \n",
    "    p_seasonality = calc_p_seasonality(df)\n",
    "    snow_frac_daily = calc_snow_frac_daily(df)\n",
    "    high_prec_freq = calc_high_prec_freq(df, p_mean=p_mean)\n",
    "    high_prec_dur = calc_high_prec_dur(df, p_mean=p_mean)\n",
    "    low_prec_freq = calc_low_prec_freq(df)\n",
    "    low_prec_dur = calc_low_prec_dur(df)\n",
    "\n",
    "    # return the dataframe with a single basin\n",
    "    return pd.DataFrame(\n",
    "        data={\n",
    "            \"basin_id\": basin_id,\n",
    "            \"p_mean\": p_mean,\n",
    "            \"p_seasonality\": p_seasonality,\n",
    "            \"frac_snow_daily\": snow_frac_daily,\n",
    "            \"high_prec_freq\": high_prec_freq,\n",
    "            \"high_prec_dur\": high_prec_dur,\n",
    "            \"low_prec_freq\": low_prec_freq,\n",
    "            \"low_prec_dur\": low_prec_dur,\n",
    "        },\n",
    "        index=[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  basin_id    p_mean  p_seasonality  frac_snow_daily  high_prec_freq  \\\n",
      "0    11001  1.317507       0.361732         0.346277       14.913043   \n",
      "\n",
      "   high_prec_dur  low_prec_freq  low_prec_dur  \n",
      "0       1.230903     243.304348      4.478745  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IF meteo_path is a directory of multiple basins --> uncomment'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with precipitation from ERA5 Land \n",
    "clim_static_df = pd.DataFrame()\n",
    "\n",
    "# processing sample basin 11001\n",
    "meteo_path = '11001.csv'\n",
    "\n",
    "'''In this example we have only one basin to process'''\n",
    "df = pd.read_csv(meteo_path, parse_dates=['date'])\n",
    "df.rename(columns={'prcp_era': 'prcp'}, inplace=True)\n",
    "\n",
    "clim_static_df = process_basin(df, meteo_path.split('.')[0])\n",
    "print(clim_static_df)\n",
    "\n",
    "\n",
    "\n",
    "'''IF meteo_path is a directory of multiple basins --> uncomment'''\n",
    "# result = []\n",
    "#\n",
    "# for file in os.listdir(meteo_path):\n",
    "#     df = pd.read_csv(meteo_path + file, parse_dates=['date'])\n",
    "#     df.rename(columns={'prcp_era': 'prcp'}, inplace=True)\n",
    "#     result.append(process_basin(df, file.split('.')[0]))\n",
    "\n",
    "# clim_static_df = pd.concat(result, axis=0)\n",
    "# print(clim_static_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
